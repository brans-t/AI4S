import os
import json
import requests
import time
from tqdm import tqdm  # è¿›åº¦æ¡æ˜¾ç¤º
from concurrent.futures import ThreadPoolExecutor  # å¹¶è¡Œä¿å­˜ MOF æ–‡ä»¶

# ----------------------------
# åŸºç¡€é…ç½®
# ----------------------------
BASE_URL = "https://mof.tech.northwestern.edu"  # MOF æ•°æ®åº“æœåŠ¡å™¨ URL
MOFS_URL = f"{BASE_URL}/mofs.json"  # è·å– MOF æ•°æ®çš„ API
DATABASES_URL = f"{BASE_URL}/databases.json"  # è·å–æ•°æ®åº“åˆ—è¡¨çš„ API

PER_PAGE = 200  # æ¯é¡µ MOF æ•°é‡
MAX_RETRIES = 3  # è¯·æ±‚å¤±è´¥æœ€å¤§é‡è¯•æ¬¡æ•°
MAX_WORKERS = 10  # å¹¶è¡Œä¿å­˜æ–‡ä»¶çº¿ç¨‹æ•°

# ----------------------------
# å·¥å…·å‡½æ•°ï¼šå®‰å…¨æ–‡ä»¶å¤¹å
# ----------------------------
def safe_name(name: str) -> str:
    """
    å°†æ•°æ®åº“åç§°æˆ– MOF ID è½¬ä¸ºå®‰å…¨æ–‡ä»¶å¤¹/æ–‡ä»¶å
    æ›¿æ¢ç©ºæ ¼å’Œæ–œæ 
    """
    return name.replace(" ", "_").replace("/", "_")

# ----------------------------
# è·å–å¯ç”¨æ•°æ®åº“åˆ—è¡¨
# ----------------------------
def fetch_databases():
    """
    ä»æœåŠ¡å™¨è·å–å¯ç”¨ MOF æ•°æ®åº“åˆ—è¡¨

    Returns:
        list: æ¯ä¸ªæ•°æ®åº“æ˜¯ä¸€ä¸ª dictï¼ŒåŒ…å«:
              - "name" (str): æ•°æ®åº“å
              - "count" (int): MOF æ•°é‡
    """
    try:
        resp = requests.get(DATABASES_URL)
        resp.raise_for_status()
        data = resp.json()
        databases = []
        for db in data:
            databases.append({"name": db["name"], "count": db.get("mofs", 0)})
        return databases
    except Exception as e:
        print(f"âŒ Failed to fetch database list: {e}")
        # å¤‡ç”¨æ•°æ®åº“åˆ—è¡¨
        return [
            {"name": "CoREMOF 2014", "count": 4764},
            {"name": "CoREMOF 2019", "count": 12020},
            {"name": "CSD", "count": 0},
            {"name": "hMOF", "count": 137953},
            {"name": "IZA", "count": 216},
            {"name": "PCOD-syn", "count": 70},
            {"name": "Tobacco", "count": 13511},
        ]

# ----------------------------
# è·å–æŒ‡å®šæ•°æ®åº“çš„æ‰€æœ‰ MOF
# ----------------------------
def get_all_mofs(database):
    """
    ä½¿ç”¨åˆ†é¡µè·å–æŸä¸ªæ•°æ®åº“çš„æ‰€æœ‰ MOF

    Args:
        database (str): æ•°æ®åº“åç§°

    Returns:
        list: MOF è®°å½•åˆ—è¡¨ï¼Œæ¯æ¡æ˜¯ dict
    """
    all_mofs = []
    page = 1
    print(f"\nFetching MOFs from database '{database}' ...")

    while True:
        url = f"{MOFS_URL}?database={database}&page={page}&per_page={PER_PAGE}"
        for attempt in range(1, MAX_RETRIES + 1):  # é‡è¯•æœºåˆ¶
            try:
                resp = requests.get(url, timeout=10)
                resp.raise_for_status()
                break
            except Exception as e:
                print(f"âš ï¸ Page {page} request failed, retry {attempt}/{MAX_RETRIES}: {e}")
                time.sleep(1)
                if attempt == MAX_RETRIES:
                    print(f"âŒ Page {page} failed after {MAX_RETRIES} retries, skipping")
                    return all_mofs

        data = resp.json()
        mofs = None
        # åˆ¤æ–­å“åº”ä¸­ MOF åˆ—è¡¨ä½ç½®
        if isinstance(data, dict):
            for k in ["mofs", "results", "data"]:
                if k in data and isinstance(data[k], list):
                    mofs = data[k]
                    break
        elif isinstance(data, list):
            mofs = data
        else:
            print(f"âŒ Unknown data structure on page {page}: {type(data)}")
            break

        if not mofs:
            print(f"âš ï¸ No MOF data found on page {page}")
            break

        all_mofs.extend(mofs)
        print(f"âœ… Page {page}: {len(mofs)} records (total {len(all_mofs)})")

        if len(mofs) < PER_PAGE:  # æœ€åä¸€é¡µ
            break
        page += 1
        time.sleep(0.01)

    print(f"ğŸ“¦ Total {len(all_mofs)} MOF records fetched")
    return all_mofs

# ----------------------------
# ä¿å­˜å•ä¸ª MOF
# ----------------------------
def save_one_mof(mof, database, save_dir):
    """
    å°†å•ä¸ª MOF ä¿å­˜ä¸º JSON å’Œ CIF æ–‡ä»¶

    Args:
        mof (dict): MOF æ•°æ®
        database (str): æ•°æ®åº“åç§°
        save_dir (str): ä¿å­˜è·¯å¾„
    """
    folder_name = safe_name(database)
    json_folder = os.path.join(save_dir, "JSON", folder_name)
    cif_folder = os.path.join(save_dir, "CIF", folder_name)
    os.makedirs(json_folder, exist_ok=True)
    os.makedirs(cif_folder, exist_ok=True)

    # MOF æ–‡ä»¶åå®‰å…¨åŒ–
    mofid = str(mof.get("mofid") or mof.get("mofkey") or mof.get("id") or mof.get("name") or "mof")
    mofid = safe_name(mofid)

    # ä¿å­˜ JSON
    json_path = os.path.join(json_folder, f"{folder_name}_{mofid}.json")
    try:
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(mof, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"âŒ Failed to save JSON for {mofid}: {e}")

    # ä¿å­˜ CIF
    cif_data = mof.get("cif")
    if cif_data:
        cif_path = os.path.join(cif_folder, f"{folder_name}_{mofid}.cif")
        try:
            with open(cif_path, "w", encoding="utf-8") as f:
                f.write(cif_data)
        except Exception as e:
            print(f"âŒ Failed to save CIF for {mofid}: {e}")

# ----------------------------
# å¹¶è¡Œä¿å­˜å¤šä¸ª MOF
# ----------------------------
def save_mofs_parallel(mofs, database, save_dir):
    """
    ä½¿ç”¨çº¿ç¨‹å¹¶è¡Œä¿å­˜ MOF

    Args:
        mofs (list): MOF æ•°æ®åˆ—è¡¨
        database (str): æ•°æ®åº“åç§°
        save_dir (str): ä¿å­˜è·¯å¾„
    """
    total = len(mofs)
    print(f"\nSaving MOFs for {database} ...")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        list(tqdm(
            executor.map(lambda m: save_one_mof(m, database, save_dir), mofs),
            total=total,
            unit="MOF",
            desc=f"Saving {database}"
        ))

    folder_name = safe_name(database)
    json_folder = os.path.join(save_dir, "JSON", folder_name)
    cif_folder = os.path.join(save_dir, "CIF", folder_name)
    json_count = len(os.listdir(json_folder)) if os.path.exists(json_folder) else 0
    cif_count = len(os.listdir(cif_folder)) if os.path.exists(cif_folder) else 0
    missing_cif = total - cif_count

    print("\nğŸ“Š Download Summary")
    print(f"Database: {database}")
    print(f"Total MOFs: {total}")
    print(f"JSON files: {json_count}")
    print(f"CIF files: {cif_count}")
    print(f"Missing or unavailable CIF files: {missing_cif}")

# ----------------------------
# ä¸‹è½½æŒ‡å®šæ•°æ®åº“
# ----------------------------
def download_databases(selected_indices=None, save_dir=None, all_databases=False):
    """
    ä¸‹è½½æŒ‡å®šæ•°æ®åº“çš„ MOF æ•°æ®
    - selected_indices: æ•°æ®åº“ç´¢å¼•åˆ—è¡¨
    - save_dir: ä¿å­˜ç›®å½•
    - all_databases: æ˜¯å¦ä¸‹è½½å…¨éƒ¨æ•°æ®åº“
    """
    if save_dir is None:
        save_dir = os.path.join(os.path.dirname(__file__), "MOF_all_raw")

    databases = fetch_databases()

    if all_databases:
        target_indices = list(range(len(databases)))
    elif selected_indices is not None:
        target_indices = selected_indices
    else:
        raise ValueError("âŒ No databases specified. Provide indices or set all_databases=True.")

    for idx in target_indices:
        if 0 <= idx < len(databases):
            db_name = databases[idx]["name"]
            print(f"\nğŸ“¥ Downloading database: {db_name}")
            mofs = get_all_mofs(db_name)
            save_mofs_parallel(mofs, db_name, save_dir)
        else:
            print(f"âš ï¸ Invalid database index {idx}, skipping.")

    print(f"\nğŸ‰ All selected databases have been downloaded to {save_dir}, with JSON and CIF stored separately.")

# ----------------------------
# è„šæœ¬å…¥å£
# ----------------------------
if __name__ == "__main__":
    download_databases(all_databases=True)
